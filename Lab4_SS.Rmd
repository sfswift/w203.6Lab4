---
title: 'Lab 4: Reducing Crime'
author: "Sullivan Swift, Jayanth Srinivasa"
date: "December 4, 2017"
output:
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(car)
library(lmtest)
library(sandwich)
library(stargazer)

#set working directory
setwd("C:/Users/Sullivan/Dropbox/W203/Lab 4")
#setwd("~/Desktop/data_w203/w203.6Lab4")

#load the data
crime <- read.csv("crime.csv")

#replicate description of variables
variable <- c("county",
              "year",
              "crmrte",
              "prbarr",
              "prbconv",
              "prbpris",
              "avgsen",
              "polpc",
              "density",
              "taxpc",
              "west",
              "central",
              "urban",
              "pctmin80",
              "wcon",
              "wtuc",
              "wtrd",
              "wfir",
              "wser",
              "wmfg",
              "wfed",
              "wsta",
              "wloc",
              "mix",
              "pctymle")

label <- c("county identifier",
           "1987",
           "crimes committed per person",
           "`probability` of arrest",
           "`probability` of conviction",
           "`probability` of prison sentence",
           "avg. sentence, days",
           "police per capita ",
           "people per sq. mile",
           "tax revenue per capita",
           "=1 if in western N.C.",
           "=1 if in central N.C.",
           "=1 if in SMSA",
           "perc. minority, 1980",
           "weekly wage, construction",
           "wkly wge, trns, util, commun",
           "wkly wge, whlesle, retail trade",
           "wkly wge, fin, ins, real est",
           "wkly wge, service industry",
           "wkly wge, manufacturing",
           "wkly wge, fed employees",
           "wkly wge, state employees",
           "wkly wge, local gov emps",
           "offense mix: face-to-face/other",
           "percent young male")

desc <- data.frame(variable, label)
```

## Introduction

To address questions regarding the determinants of crime in North Carolina in 1987, we conducted an analysis of the state's crime rate and possible related varaibles, including many of the following:

```{r}
#description of the variables in the crime dataset
desc
#nmber of rows
nrow(crime)
summary(crime)
```

To address the question regarding the causes of crime, we examined variables crime rate (`crmrte`), density per square mile (`density`), tax revenue per capita (`taxpc`), percent minority in 1980 (`pctmin80`), percent young male (`pctymle`), and probability of arrest (`prbarr`) as our main variables of interest. We also wanted to examine wage data, so we calculated the median weekly wage (`med_wag`) for each county. We believe these variables will give us a wholistic view of each county in terms of population density, demographics, and wealth.

First, we created our new `med_wag` variable, and performed a high level analysis to assess the quality of our data.

```{r}
#Take the median for each county from all wage variables.
crime$med_wag <- apply(crime[,(16:24)], 1, median, na.rm=TRUE)

#check for NAs among key variables
C <- crime
filter = !is.na(C$crmrte) | !is.na(C$density) | !is.na(C$taxpc) | !is.na(C$pctmin80) | !is.na(C$pctymle) | !is.na(C$med_wag)
C = C[filter,]
summary(C)
nrow(C)

#review the aggregate location data
table(crime$west)
table(crime$central)
table(crime$urban)
summary(crime$county)
```

There are no NA values, so we were able to continue with the complete dataset. Above we briefly examined location data. We originally wanted to incorporate location into the analysis, but these three variables did not create groups of even $n$ and the `county` variable is too granular on its own, so we did not include location data in the analysis.

```{r}
#examine key outcome variable, crmrte
summary(crime$crmrte)
hist(crime$crmrte, breaks=50,
     main="Histogram of crmrate")

#apply log transformation
summary(log(crime$crmrte))
hist(log(crime$crmrte), breaks=50,
     main="Histogram of log(crmrte)")

#store transformation
crime$log_crmrte <- log(crime$crmrte)
```

First we analyzed the outcome variable, crime rate (`crmrte`). In the original `crmrte` variable, the distribution is right tailed. We applied a log transformation to `crmrte`, and this variable had a more normal distribution. We chose to use the log of `crmrte` as our outcome variable in the models below.

```{r}
#examine predictor variable, density
summary(crime$density)
hist(crime$density, breaks=50,
     main="Histogram of Density per sq mile")

#apply log transformation
summary(log(crime$density))
hist(log(crime$density), breaks=50,
     main="Histogram of log(density)")

#store transformation
crime$log_density <- log(crime$density)
```

Next we examined density per square mile (`density`). There is a positive skew, so we again applied a log transformation. The log transformation of `density` is not quite normal, but since our $n=90$, we can rely on the Central Limit Theorem.

```{r}
#examine predictor variable, taxpc
summary(crime$taxpc)
hist(crime$taxpc, breaks=50,
     main="Histogram of Tax Revenue per Capita")

#examine predictor variable, med_wag
summary(crime$med_wag)
hist(crime$med_wag, breaks=50,
     main="Histogram of Median Wages")
```

We then analyzed tax revenue per capita (`taxpc`) and median wage (`med_wag`). We were concerned that these two variables have too much overlap in effect - wages are likely a very similar measure to the tax revenue per capita. After looking at the histograms of these two variables, we chose `med_wage` over `taxpc` because it is closer to a normal distribution on its own and will be more clear to interpret in our models below.

```{r}
#examine predictor variable, pctmin80
summary(crime$pctmin80)
hist(crime$pctmin80, breaks=50,
     main="Histogram of Percent Minority in 1980")

#apply log transformation
summary(log(crime$pctmin80))
hist(log(crime$pctmin80), breaks=50,
     main="Histogram of log(pctmin80)")
```

Next, we looked at percent minority in 1980 (`pctmin80`). This data's distribution was not normal, so we applied a log transformation. This caused a left skew to the data, and we chose to continue with the untransformed variable, relying on the Central Limit Theorem for normality.

```{r}
#examine predictor variable, pctymle
summary(crime$pctymle)
hist(crime$pctymle, breaks=50,
     main="Histogram of Percent Young Male")

#apply log transformation
summary(log(crime$pctymle))
hist(log(crime$pctymle), breaks=50,
     main="Histogram of log(pctymle)")

#store transformation
crime$log_pctymle <- log(crime$pctymle)
```

Then we examined the percentage of young males variable (`pctymle`). The values were all very small, between .06 and .25, and it appears to be in a decimal percentage format. `pctymle` also has a right skew. To address the skew, we used a log transformed `pctymle`. Using a log transformation we can also more easily interpret `pctymle` below in our models. 

```{r}
#examine predictor variable, prbarr
summary(crime$prbarr)
hist(crime$prbarr, breaks=50,
     main="Histogram of Probability of Arrest")

#apply log transformation
summary(log(crime$prbarr))
hist(log(crime$prbarr), breaks=50,
     main="Histogram of log(prbarr)")

#store transformation
crime$log_prbarr <- log(crime$prbarr)
```

Lastly, we examined the probability of arrest (`prbarr`). While it is not as skewed as the rest, log(`prbarr`) is still slightly skewed to the right and has one fairly large outlier, so we decided to apply the log transform to this variable. The log transformation has a much more normal distribution.

```{r}
#scatterplot matrix
scatterplotMatrix(crime[,c('crmrte', 'density', 'med_wag', 'taxpc', 'pctmin80', 'pctymle', 'prbarr')])

#correlation matrix
(c <- with(crime, cor(cbind(crmrte, density, log_density, med_wag, taxpc, pctmin80, pctymle,log_pctymle, prbarr))))
```

Finally we examined a scatterplot matrix and correlation matrix to quickly assess the relationships between our variables. None of the variables we examined have a perfect correlation, though some have fairly strong relationships. Density has a strong correlation with the `crmrte`. We used `density` as a key variable in our models below for its intutive relationship to `crmrte` as well as the corrlational relationship. We chose to include `med_wag`, but not `taxpc`, as `med_wag` has a more normal distribution and has a more clear interpretation. We chose to include `pctmin80`, and the transformation log(`pctymle`) over `pctymle`. We can use this correlation matrix to confirm MLR3, no perfect multicollinearity, for our models below.

## Modeling Crime Rate and Addressing Assumptions

Addressing CLM1/MLR1, the linearity assumption, the three models we create below are linear in nature, so we meet this assumption.

Our sample appears to be nearly the entire population, and we ran into no NA values above. North Carolina has 100 counties, and our dataset contains 90. Enough of the population is included in the dataset for us to assume CLM2/MLR2, the assumption of random sampling is satisfied.

We addressed MLR3 above.

### Model 1 

We defined our first model hoping to use only one or two key variables. We settled on log(`crmrte`) ~ log(`density`), as the more densely populated an area, the opportunity for crime to occur increases both from increased individuals and increased property in an area. Above we also see that `density` has a strong, positive correlation with `crmrte`.

```{r}
#create the model
(model1 <- lm(log_crmrte ~ log_density, data=crime))
```

First, we create the model (`model1`) and examine its coefficients. For each 1% increase in `density`, there is a .49% increase in `crmrte`.

```{r}
#examine the covariance between the predictor and the residuals
cov(crime$log_density, model1$residuals)
```

Examining the covariance, we see a very small relationship between `log_density` and `model1`'s residuals. This supports exogeneity. 

```{r}
#resdiuals v fitted values plot
plot(model1, which=1)
```

Next we examine the Residuals v Fitted plot to assess MRL4, the zero-conditional mean assumption. We see that the red spline line is very close to zero through the whole graph. The exception is the small uptick on the far left, but this is likely due to few data points.

```{r}
#scale-location plot
plot(model1, which=3)

#breusch-pagan test
bptest(model1)
#score-test
ncvTest(model1)

#heteroskedatic robust standard errors
se.model1 = sqrt(diag(vcovHC(model1)))
```

To assess MLR5, the homoskedasticity assumption, we look at the Residuals v Fitted plot above, as well as the Scale-Location plot. On both plots, the band of data points narrows as we move right on the graphs, providing evidience of a violation of homoskedasticity. Further supporting this violation, the Breusch-Pagan test and the Score-test both have significant p-values (bptest, $p=.01$; score-test, $p=.005$). Our sample is $n>30$, however the sample is not extremely large. We will use the robust standard errors we have produced above to address the posibility of heteroskedasticity.

```{r}
#residuals vs leverage plot
plot(model1, which=5)
```

Throughout out analysis above, we noticed there was at least one quite large outlier. Here we looked at the Residuals v Leverage plot to determine if we need to remove this data point. The outlier fall inside of the Cook's distnace and does not appear to have any sizable bearing on `model1`, so we may leave it in our analysis.

```{r}
#qqplot
plot(model1, which=2)

#histogram of Model1 residuals
hist(model1$residuals, breaks=50,
     main="Histogram of Model1 Residuals")
```

When assessing MLR6, we examine the Q-Q plot of the model. Here, we can see that the data closely hugs the diagonal, indicating normal residuals. The histogram of the residuals also supports this conclusion. Both charts a few values on the extremes that vary, but on the whole support MLR6.

Next, we ran a summary on `model1` and used our standard errors that are robust to heteroskedasticity.

```{r}
(s1 <- summary(model1))
s1$coefficients[, 2] <- sqrt(diag(vcovHC(model1)))
s1
```

We noticed that the R-squared value was `s1$r.squared`, which is a reasonable value. 

```{r}
coeftest(model1, vcov = vcovHC)
```


### Model 2 

Next, we wanted to expand our model with more independent variables, and we chose to add `pctymle` and `pctmin80` incorporate some demographic information in our model. Since the `pctymle` distribution was skewed from the analysis of the histogram, we decided to use the log form of the variable in the analysis. The percent of young males (`pctymle`) and the percent minority in 1980 (`pctmin80`) are both reasonable variables to include in this model as they tell us about the demographic diversity of a county. Though the `pctmin80` data is 7 years old, we chose to include it in our model so we could somewhat assess the impact of minorities on crime rate. 


```{r}
#create the model
(model2 <- lm(log_crmrte ~ log_density + log_pctymle + pctmin80, data=crime))
```

From examining the coefficients we see that, with everything else being held a constant an increase of 1% in `density`, we see an increase of 0.5% in the crime rate. Similary, an increase in `pctymle` with everything else being constant, produces an increase of crime rate by 0.38% and finally any increase of minority population by 1 unit, results in a nearly 1% increase in the crime rate. 

Now let us check the remaining assumptions. First, we will double check MLR3 with the variable's Variance Inflation Factors. Then we will examine the zero conditional mean assumption by looking at the plot of residuals vs fitted values.


```{r}
#Variance Inflation Factors
vif(model2)

#residuals v fitted values plot
plot(model2, which=1)
```

The VIF values are low, less than 10, supporting the MLR3 assumption. The red spline line is close to zero for all the fitted values, with only a small downturn on the left side. Compared to `model1`, the red spline line appears to fit zero better. This supports making the MLR4 assumption.

```{r}
#examine covariance of predictor variables and model residuals
cov(crime$log_density, model2$residuals)
cov(crime$log_pctymle, model2$residuals)
cov(crime$pctmin80, model2$residuals)
```

The covariance between each of the independent variables and the model residuals are each very small, and it is clear that exogenity assumption holds.

To assess homoskedasticity, we looked at the graph of standardized residuals vs the fitted values below.

```{r}
#scale-location plot
plot(model2, which=3)

#breusch-pagan test
bptest(model2)
#score test
ncvTest(model2)

#heteroskedastic robust standard errors
se.model2 = sqrt(diag(vcovHC(model2)))
```

The graph shows some outliers to the upper left edge and the band seems to narrow as we move to the right, indicating there may be heteroskedasticity.

The Breusch-Pagan test did not yeild significant results ($p>.05$), but the Non-constant Variance Score Test was significant with $p<.001$. We will be conservative and use the robust standard errors we have produced above to address any heteroskedacity in the data, just as in `model1`.

To test the normality of the errors, we examined the Q-Q plot of the residuals and the histogram of the residuals.

```{r}
#qq plot
plot(model2, which=2)

#histogram of model2 residuals
hist(model2$residuals, breaks=50,
     main="Histogram of Model2 Residuals")
```

The Q-Q plot has some variation around the ends, and both the histogram and the Q-Q plot contain an outlier to the far right. The histogram of the residuals is fairly normal, except for the outlier on the far right. `model2` has more variation in the residuals than `model1`, but the evidence is strong enough to support making the MLR6 assumption.

Finally we can examine the summary command, using our heteroskedasticity robust standard errors from above.

```{r}
(s2 <- summary(model2))
s2$coefficients[, 2] <- sqrt(diag(vcovHC(model2)))
s2
```

That the rsquare value has improved to `s2$r.squared` and the adjusted R-square is `s2$adj.r.squared` in Model2. This indicates a better performance compared to Model1.

```{r}
coeftest(model2, vcov = vcovHC)
```

### Model 3

We created one more model to include the previous covariates and along with the probability of arrests (`prbarr`) and the median wage (`med_wag`) to assess the robustness of our first models.
  
```{r}
#create the model
(model3 <- lm(log_crmrte ~ log_density + log_pctymle + pctmin80 + med_wag + log_prbarr, data=crime))
```

From the `model3`, we can say that as the population density increases by 1%, the crime rate goes up by 0.4799% with everything else being constant. Similarly, a percentage increase in the fraction of young males in the population by 1% increases the crime rate by 0.27% and an increase in minorities by 1% (compared to their population in 1980), causes an increase in crime-rate by 1.18%. Finally, the two new variables `med_wage` and probability of arrests (`prbarr`), both have a negative effect on the crime rate. And as median wage increases by a unit of 1, the crime-rate decreases by 0.5% and as the probability of arrest goes up by 1%, the crime rate goes down by 0.24%.  

As before, we confirm MLR3 with VIF and examine the plot of residuals vs fitted values to determine the the zero conditional mean assumption.

```{r}
#Variance Inflation Factors
vif(model3)

#residuals vs fitted values plot
plot(model3, which=1)
```

Again, all VIF values are less than 10, supporting the MLR3 assumption. There is no evidence of a violation of MLR4. While the red spline line very close to zero, we also observe that to the left the line seems to move further below zero than in `model2`. This can likely be attributed to fewer data points on the left side of the fitted values.

```{r}
#examine covariance of predictor variables and model residuals
cov(crime$log_density, model3$residuals)
cov(crime$log_pctymle, model3$residuals)
cov(crime$pctmin80, model3$residuals)
cov(crime$med_wag, model3$residuals)
cov(crime$log_prbarr, model3$residuals)
```

Again, the covariance between each of the independent variables and the model residuals are very small, and we see exogeneity holds.

Next we look at `model3`'s scale-location plot, Breusch-Pagan test, and the Non-constant Variance Score Test to assess homoskedasticity.

```{r}
#scale-location plot
plot(model3, which=3)
#breusch-pagan test
bptest(model3)
#score test
ncvTest(model3)

#heteroskedastic robust standard error
se.model3 = sqrt(diag(vcovHC(model3)))
```

The graph clearly shows there are some outliers, showing the variables are heteroskedastic. Further supporting this violation, the Non-constant Variance Score Test test has significant p-value ($p<.01$). The Breusch-Pagan test did not yeild significant results ($p>.05$) We will be conservative and use the robust standard errors we have produced above to address any heteroskedasticity in the data.

Again, we examine a Q-Q plot of the residuals and the histogram of the residuals to determine the MLR6 assumption.

```{r}
#qq plot
plot(model3, which=2)

#histogram of model3 residuals
hist(model3$residuals, breaks=50,
     main="Histogram of Model3 Residuals")
```

Both plots show a fairly normal distribution, with a smaller outlier to the left and a larger outlier to the right. `model3`'s residuals histrogram looks marginally more normal than `model2`'s, and MLR6 still holds.

Finally we examine the summary of `model3` with residuals robust to heteroskedasticity.

```{r}
s3 <- summary(model3) 
s3$coefficients[, 2] <- sqrt(diag(vcovHC(model3)))
s3
```

Though the rsquare value has improved to `s3$r.squared` and the adjusted R-square is `s3$adj.r.squared` in `model3`, it hasn't improved all that much in comparison to `model2` and does not justify the addition of the extra variables.


```{r}
coeftest(model3, vcov = vcovHC)
```

## Comparison of Models

We used the Stargazer library function to compare our three models. Though we see little evidence of complete heteroskedasticity, we would like to be conservative and we are using the robust standard errors for all the three models.

```{r}
#model comparison output with stargazer
stargazer(model1, model2, model3, type = "text", omit.stat = "f",
          se = list(se.model1, se.model2, se.model3),
          star.cutoffs = c(0.05, 0.01, 0.001))
```

For `model1`, we see that the `log_density` is statistically significant at the $p<.001$ level. Also since we are examining the elastic relationship between the crime rate and the density of population, we see that for 1% increase in the density of population results in an increase in crmrte bye 0.486%. Since this is a pretty observable effect, the log_density is practically significant too.

For `model2`, we see that `log_density`, `log_pctymle`, and `pctmin80` are all statistically significant at the $p<.001$ level. This clearly indicates statistical significance. Also, given everything else is constant, the crime rate increases by 0.5% with every 1% increase in the density of population, increases by 0.38% for every 1% increase in the fraction of the young male population and finally the crime rate will increase by 1.2% when the minority population increases by 1% over the 1980 value. The value of change signifies a practical significance too.

For `model3`, we see that the `log_density` and `pctmin80` are statistically significant at the $p<0.001$ level, `log_pctymle` is statistically significant at the 0.01 level, but `med_wag` and `log_prbarr` are not statistically significant. Practically, holding everything else constant, just increasing the density of the population by 1% increases the crime rate by 0.48%, only increasing the young male population by 1% causes an increase in crime rate by 0.27%, increasing the minority population by 1% increase crime rate by 1.2%. The median wage and the `log_prbarr` (log of probability of arrest) have a negative effect on the crime rate. That is as the median wage increases by 1 unit, the crime decreases by 1% and while the probability of arrest increases by 1%, the crime rate decreases by 0.25%.

From the adjusted r-squared values we can clearly see an improvement from `model1` to `model2`. However, while there is a modest increase in `model3` when compared to `model2`, it does not justify the addition of two variables that are statistically insignificant.


## Omitted Variables

Note that we did not analyze police per capita (`polpc`), probability of conviction (`prbconv`), probability of prison sentence (`prbpris`), average sentence (`avgsen`), or the offense mix (`mix`). There are several reasons we excluded `polpc` from our analysis. First, the number of police officers may be highly dependent on how much crime occurs in an area, and second, a county's resources, or wealth, could impact the number of police. Since we already have measures for wealth, and due to this possible confounding relationship between police and crime, we excluded the `polpc` variable from analysis. `avgsen` and `mix` are more along the lines of a crime outcome, rather than a predictor of crime in an area. We chose between `prbarr`, `prbcon`, and `prbpris` as these variables measure similar events and including all three would be redundant, so we just include the transformed `prbarr` variable. We also briefly analyze aggregate location variables (`west`, `central`, `urban`), but these did not cut the data into groups of similar sizes.

The dataset also does not contain any variables that directly correspond to employability or employment availability in the counties.  While, there are financial variables, it is reasonable to assume there is a relationship between crime and employment in an area. This creates a bias and since it is sensible to infer that it employability and employment availability would add to the bias of all the independent variables - `log_density`, `log_pctymle`, `pctmin80`, `log_prbarr`, and `med_wag`. 

Let us look at them one by one.  Employability and employment availability in a county would likely increase the density of population and hence would lead to a positive bias and lead to overstating the effects of the density in the model. Same would be the case with the percent of young males, as it is realistic to assume that more young men would be a higher percentage of the population when there are more jobs. Given this to be true, our model will overstate the impact of density and the percentage of young males.  The minority population is a similar case.  The median wage would depend on the type and number jobs in an area, driving the median wage up or down. In general, it is reasonable to assume that the omitted variable of employment and employment availability would increase the impact of median wage. Finally, the probability of arrests can be reasonably expected to be negatively correlated with employment/employability and this would mean that we are understating the bias of the probability of arrest. Also, we would expect employment opportunities/employability to be negatively correlated with crime rate in general and this will lead to a negative bias on the constant. 

## Conclusions and Implications

From our analysis, we can see that the variables `log_density`, `log_pctymle` and `pctmin80` had the biggest impact on `crmrte`. Although these results are informative, no causal relationship can be drawn from our models. In general, regression itself does not imply causality; it's basis is in correlation. In this instance, this general rule holds. The data we have is observational, and no experiment was performed. However, we can still see that population density, demographics, and crime rate have a relationship.

Also in `model3`, we observe that the median wage has a negative slope (while not significant statistically), we can hypothesize that increasing the median wage may decrease crime rate.

Now, since we cannot reduce the density of population in counties, we may need to look at the reasons for increased density, causing an increase in crime rate. It may be a good idea to ensure economic development to reduce the crime rate.

While there is a positive relation between the crime rate and demographic diversity of an area, this does not imply diversity has a negative impact on society. Instead, creating programs to aid underprivileged families and expanding the availability of education would not only help these individuals, but also improve society as a whole and potentially reduce crime.

In conclusion, the crime rate in North Carolina is a complex issue dependent on many variables - some of which cannot be controlled or even observed. However, there are measure we can take improve the social and economic status of an area to reduce the impact of factors that may contribute to an increased crime rate.
